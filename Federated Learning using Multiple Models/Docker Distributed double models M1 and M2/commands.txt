
Build:
docker build -t server -f Dockerfile.server .
docker build -t clienta -f Dockerfile.clienta .
docker build -t clientb -f Dockerfile.clientb .


Run:
docker network create mynetwork
docker run -d --name server -p 8080:8080 server
docker network connect mynetwork server


Test 4 Clients
docker run -d --name clienta-1 --network mynetwork --cpus="2.6" clienta
docker run -d --name clienta-2 --network mynetwork --cpus="2.6" clienta

docker run -d --name clientb-1 --network mynetwork --cpus="2.6" clientb
docker run -d --name clientb-2 --network mynetwork --cpus="2.6" clientb


Test 6 Clients
docker run -d --name clienta-1 --network mynetwork --cpus="1.5" clienta
docker run -d --name clienta-2 --network mynetwork --cpus="1.5" clienta
docker run -d --name clienta-3 --network mynetwork --cpus="1.5" clienta

docker run -d --name clientb-1 --network mynetwork --cpus="1.5" clientb
docker run -d --name clientb-2 --network mynetwork --cpus="1.5" clients
docker run -d --name clientb-3 --network mynetwork --cpus="1.5" clientb







TEST
# Client.py
from flwr.client import ClientApp, NumPyClient
from flwr.common import (
    parameters_to_ndarrays,
    ndarrays_to_parameters,
    Scalar,
    Context,
)
from typing import Dict
import time
import hashlib
import os
import psutil
import torch
from taskA import DEVICE as DEVICE_A, Net as NetA, get_weights as get_weights_A, load_data as load_data_A, set_weights as set_weights_A, train as train_A, test as test_A
from taskB import DEVICE as DEVICE_B, Net as NetB, get_weights as get_weights_B, load_data as load_data_B, set_weights as set_weights_B, train as train_B, test as test_B

from APClient import ClientRegistry

client_registry = ClientRegistry()

DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

performance_dir = './performance/'
if not os.path.exists(performance_dir):
    os.makedirs(performance_dir)

csv_file = os.path.join(performance_dir, 'FLwithAP_performance_metrics.csv')
if not os.path.exists(csv_file):
    with open(csv_file, 'w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['Client ID', 'FL Round', 'Training Time', 'Communication Time', 'Total Time', 'CPU Usage (%)', 'Task'])


class FlowerClient(NumPyClient):
    def __init__(self, cid):
        self.cid = cid
        self.model_type = client_registry.get_client_model(self.cid)

        if self.model_type == "taskA":
            self.net = NetA().to(DEVICE_A)
            self.trainloader, self.testloader = load_data_A()
            self.device = DEVICE_A
        elif self.model_type == "taskB":
            self.net = NetB().to(DEVICE_B)
            self.trainloader, self.testloader = load_data_B()
            self.device = DEVICE_B

    def fit(self, parameters, config):
        print(f"CLIENT {self.cid} ({self.model_type}): Starting training.", flush=True)
        cpu_start = psutil.cpu_percent(interval=None)

        comm_start_time = time.time()

        if self.model_type == "taskA":
            set_weights_A(self.net, parameters)
            results, training_time = train_A(self.net, self.trainloader, self.testloader, epochs=1, device=self.device)
            new_parameters = get_weights_A(self.net)
        elif self.model_type == "taskB":
            set_weights_B(self.net, parameters)
            results, training_time = train_B(self.net, self.trainloader, self.testloader, epochs=1, device=self.device)
            new_parameters = get_weights_B(self.net)

        comm_end_time = time.time()

        cpu_end = psutil.cpu_percent(interval=None)
        cpu_usage = (cpu_start + cpu_end) / 2

        communication_time = comm_end_time - comm_start_time
        total_time = training_time + communication_time

        metrics = {
            "train_loss": results["train_loss"],
            "train_accuracy": results["train_accuracy"],
            "val_loss": results["val_loss"],
            "val_accuracy": results["val_accuracy"],
            "training_time": training_time,
            "communication_time": communication_time,
            "total_time": total_time,
            "cpu_usage": cpu_usage,
            "client_id": self.cid,  # Use the 4-character cid
            "model_type": self.model_type,
        }

        return new_parameters, len(self.trainloader.dataset), metrics

    def evaluate(self, parameters, config):
        print(f"CLIENT {self.cid} ({self.model_type}): Starting evaluation.", flush=True)

        if self.model_type == "taskA":
            set_weights_A(self.net, parameters)
            loss, accuracy = test_A(self.net, self.testloader)
        elif self.model_type == "taskB":
            set_weights_B(self.net, parameters)
            loss, accuracy = test_B(self.net, self.testloader)
        else:
            raise ValueError(f"Unknown model type: {self.model_type}")

        print(f"CLIENT {self.cid} ({self.model_type}): Evaluation completed", flush=True)
        metrics = {
            "accuracy": accuracy,
            "client_id": self.cid,  # Use the 4-character cid
            "model_type": self.model_type,
        }
        return loss, len(self.testloader.dataset), metrics

def client_fn(context: Context):
    original_cid = str(context.node_id)
    # Hash the original_cid to create a short cid
    hash_object = hashlib.md5(original_cid.encode())
    cid = hash_object.hexdigest()[:4]
    return FlowerClient(cid=cid).to_client()

app = ClientApp(client_fn=client_fn)

if __name__ == "__main__":
    from flwr.client import start_client

    original_cid = "1234567890"
    original_cid_str = str(original_cid)

    hash_object = hashlib.md5(original_cid_str.encode())
    cid = hash_object.hexdigest()[:4]

    start_client(
        server_address="127.0.0.1:8080",
        client=FlowerClient(cid=cid),
    )


Server
from typing import List, Tuple, Dict, Optional
from flwr.common import (
    Metrics,
    ndarrays_to_parameters,
    parameters_to_ndarrays,
    Parameters,
    FitRes,
    EvaluateRes,
    Scalar,
    Context,
    FitIns,
    EvaluateIns,
)
from flwr.server import (
    ServerConfig,
    ServerApp,
    ServerAppComponents,
)
from flwr.server.strategy import Strategy
from flwr.server.client_manager import ClientManager
from flwr.server.client_proxy import ClientProxy

from taskA import Net as NetA, get_weights as get_weights_A
from taskB import Net as NetB, get_weights as get_weights_B
import time
import csv
import os
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
from APClient import ClientRegistry
import psutil

client_registry = ClientRegistry()

global_metrics = {
    "taskA": {"train_loss": [], "train_accuracy": [], "val_loss": [], "val_accuracy": []},
    "taskB": {"train_loss": [], "train_accuracy": [], "val_loss": [], "val_accuracy": []},
}

# Set the non-interactive backend of matplotlib
matplotlib.use('Agg')
current_dir = os.path.abspath(os.path.dirname(__file__))

currentRnd = 0
num_rounds = 2

performance_dir = './performance/'
if not os.path.exists(performance_dir):
    os.makedirs(performance_dir)

csv_file = os.path.join(performance_dir, 'FLwithAP_performance_metrics.csv')
if os.path.exists(csv_file):
    os.remove(csv_file)

with open(csv_file, 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['Client ID', 'FL Round', 'Training Time', 'Communication Time', 'Total Time', 'CPU Usage (%)', 'Task'])


def measure_communication_time(start_time, end_time):
    communication_time = end_time - start_time
    print(f"Communication time: {communication_time:.2f} seconds")
    return communication_time


def log_round_time(client_id, fl_round, training_time, communication_time, cpu_usage, model_type):
    total_time = training_time + communication_time
    print(
        f"CLIENT {client_id} ({model_type}): Round {fl_round} completed with total time {total_time:.2f} seconds and CPU usage {cpu_usage:.2f}%"
    )

    with open(csv_file, 'a', newline='') as file:
        writer = csv.writer(file)
        writer.writerow([client_id, fl_round, training_time, communication_time, total_time, cpu_usage, model_type])

    client_registry.update_client(client_id, True)

# Function to generate performance graphs
def generate_performance_graphs():
    sns.set_theme(style="ticks")

    df = pd.read_csv(csv_file)

    unique_clients = df['Client ID'].unique()
    client_mapping = {original_id: f"client {i + 1}" for i, original_id in enumerate(unique_clients)}
    df['Client ID'] = df['Client ID'].map(client_mapping)

    num_clients = len(unique_clients)
    df = df.reset_index(drop=True)
    df['FL Round'] = (df.index // num_clients) + 1
    df[['Training Time', 'Communication Time', 'Total Time']] = df[['Training Time', 'Communication Time', 'Total Time']].round(2)
    df.to_csv(csv_file, index=False)

    plt.figure(figsize=(12, 6))
    df_melted = df.melt(id_vars=["Client ID"], value_vars=["Training Time", "Communication Time", "Total Time"],
                        var_name="Metric", value_name="Time (seconds)")
    sns.barplot(x="Metric", y="Time (seconds)", hue="Client ID", data=df_melted)
    plt.title('Performance Metrics per Client', fontweight='bold')
    plt.ylabel('Time (seconds)', fontweight='bold')
    plt.legend(title='Client ID', title_fontsize='13', fontsize='10', loc='best', frameon=True)
    plt.tight_layout()

    graph_path = os.path.join(performance_dir, 'performance_metrics.pdf')
    plt.savefig(graph_path, format="pdf")
    plt.close()

# Function to generate CPU usage graph
def generate_cpu_usage_graph():
    sns.set_theme(style="ticks")
    df = pd.read_csv(csv_file)

    plt.figure(figsize=(12, 6))
    sns.barplot(x="Client ID", y="CPU Usage (%)", data=df)
    plt.title('CPU Usage per Client', fontweight='bold')
    plt.ylabel('CPU Usage (%)', fontweight='bold')
    plt.xlabel('Client ID', fontweight='bold')
    plt.tight_layout()

    cpu_graph_path = os.path.join(performance_dir, 'cpu_usage_per_client.pdf')
    plt.savefig(cpu_graph_path, format="pdf")
    plt.close()

# Function to generate total time graph
def generate_total_time_graph():
    sns.set_theme(style="ticks")
    df = pd.read_csv(csv_file)

    plt.figure(figsize=(12, 6))
    sns.lineplot(x='FL Round', y='Total Time', hue='Client ID', data=df, marker="o", markersize=8)
    plt.title('Total Time per Round per Client', fontweight='bold')
    plt.ylabel('Total Time (seconds)', fontweight='bold')
    plt.xlabel('FL Round', fontweight='bold')
    plt.tight_layout()

    line_graph_path = os.path.join(performance_dir, 'totalTime_round.pdf')
    plt.savefig(line_graph_path, format="pdf")
    plt.close()

# Function to generate training time graph
def generate_training_time_graph():
    sns.set_theme(style="ticks")
    df = pd.read_csv(csv_file)

    plt.figure(figsize=(12, 6))
    sns.lineplot(x='FL Round', y='Training Time', hue='Client ID', data=df, marker="o", markersize=8)
    plt.title('Training Time per Round per Client', fontweight='bold')
    plt.ylabel('Training Time (seconds)', fontweight='bold')
    plt.xlabel('FL Round', fontweight='bold')
    plt.tight_layout()

    line_graph_path = os.path.join(performance_dir, 'trainingTime_round.pdf')
    plt.savefig(line_graph_path, format="pdf")
    plt.close()

# Function to generate communication time graph
def generate_communication_time_graph():
    sns.set_theme(style="ticks")
    df = pd.read_csv(csv_file)

    plt.figure(figsize=(12, 6))
    sns.lineplot(x='FL Round', y='Communication Time', hue='Client ID', data=df, marker="o", markersize=8)
    plt.title('Communication Time per Round per Client', fontweight='bold')
    plt.ylabel('Communication Time (seconds)', fontweight='bold')
    plt.xlabel('FL Round', fontweight='bold')
    plt.tight_layout()

    line_graph_path = os.path.join(performance_dir, 'communicationTime_round.pdf')
    plt.savefig(line_graph_path, format="pdf")
    plt.close()


def weighted_average_global(metrics: List[Tuple[int, Metrics]], task_type: str) -> Metrics:
    examples = [num_examples for num_examples, _ in metrics]
    total_examples = sum(examples)
    if total_examples == 0:
        return {
            "train_loss": float('inf'),
            "train_accuracy": 0.0,
        }

    train_losses = [num_examples * m["train_loss"] for num_examples, m in metrics]
    train_accuracies = [num_examples * m["train_accuracy"] for num_examples, m in metrics]
    val_losses = [num_examples * m["val_loss"] for num_examples, m in metrics]
    val_accuracies = [num_examples * m["val_accuracy"] for num_examples, m in metrics]

    avg_train_loss = sum(train_losses) / total_examples
    avg_train_accuracy = sum(train_accuracies) / total_examples
    avg_val_loss = sum(val_losses) / total_examples
    avg_val_accuracy = sum(val_accuracies) / total_examples

    # Store metrics in the global dictionary
    global_metrics[task_type]["train_loss"].append(avg_train_loss)
    global_metrics[task_type]["train_accuracy"].append(avg_train_accuracy)
    global_metrics[task_type]["val_loss"].append(avg_val_loss)
    global_metrics[task_type]["val_accuracy"].append(avg_val_accuracy)

    for num_examples, m in metrics:
        client_id = m.get("client_id")
        model_type = m.get("model_type")
        training_time = m.get("training_time")
        communication_time = m.get("communication_time")
        cpu_usage = m.get("cpu_usage")

        if client_id:
            if not client_registry.is_registered(client_id):
                client_registry.register_client(client_id, {})

            # Log including model_type
            log_round_time(client_id, currentRnd, training_time, communication_time, cpu_usage, model_type)

    return {
        "train_loss": avg_train_loss,
        "train_accuracy": avg_train_accuracy,
        "val_loss": avg_val_loss,
        "val_accuracy": avg_val_accuracy,
    }

# Initialize weights separately for taskA and taskB
parametersA = ndarrays_to_parameters(get_weights_A(NetA()))
parametersB = ndarrays_to_parameters(get_weights_B(NetB()))

def print_final_results():
    # Collect clients for each task using cid
    clients_taskA = [cid for cid, model in client_model_mapping.items() if model == "taskA"]
    clients_taskB = [cid for cid, model in client_model_mapping.items() if model == "taskB"]

    print("\nFinal results for taskA:")
    print(f"  Clients: {clients_taskA}")
    print(f"  Train loss: {global_metrics['taskA']['train_loss']}")
    print(f"  Train accuracy: {global_metrics['taskA']['train_accuracy']}")
    print(f"  Val loss: {global_metrics['taskA']['val_loss']}")
    print(f"  Val accuracy: {global_metrics['taskA']['val_accuracy']}")

    print("\nFinal results for taskB:")
    print(f"  Clients: {clients_taskB}")
    print(f"  Train loss: {global_metrics['taskB']['train_loss']}")
    print(f"  Train accuracy: {global_metrics['taskB']['train_accuracy']}")
    print(f"  Val loss: {global_metrics['taskB']['val_loss']}")
    print(f"  Val accuracy: {global_metrics['taskB']['val_accuracy']}")

# Initialize the client_model_mapping dictionary
client_model_mapping = {}

# Map to keep track of original client IDs
cid_to_original_id = {}

# Definition of the custom strategy
class MultiModelStrategy(Strategy):
    def __init__(self, initial_parameters_a: Parameters, initial_parameters_b: Parameters):
        self.parameters_a = initial_parameters_a
        self.parameters_b = initial_parameters_b

    def initialize_parameters(self, client_manager: ClientManager) -> Optional[Parameters]:
        # Return None because we use separate initial parameters for A and B
        return None

    def configure_fit(
        self,
        server_round: int,
        parameters: Parameters,
        client_manager: ClientManager,
    ) -> List[Tuple[ClientProxy, FitIns]]:
        clients = client_manager.sample(num_clients=client_manager.num_available())
        fit_configurations = []

        for client in clients:
            client_id = client.cid
            model_type = client_model_mapping.get(client_id)

            if model_type == "taskA":
                fit_ins = FitIns(self.parameters_a, {})
            elif model_type == "taskB":
                fit_ins = FitIns(self.parameters_b, {})
            else:
                # Assign default model_type or handle new clients
                # For this example, we'll assign "taskA" to new clients
                model_type = "taskA"
                client_model_mapping[client_id] = model_type
                fit_ins = FitIns(self.parameters_a, {})

            fit_configurations.append((client, fit_ins))

        return fit_configurations

    def aggregate_fit(
            self,
            server_round: int,
            results: List[Tuple[ClientProxy, FitRes]],
            failures: List[BaseException],
        ) -> Optional[Tuple[Parameters, Dict[str, Scalar]]]:
            # Separate results per model type
            results_a = []
            results_b = []

            for client_proxy, fit_res in results:
                # Use client_id from metrics
                client_id = fit_res.metrics.get("client_id")  # This should be the 4-character cid
                model_type = fit_res.metrics.get("model_type")
                client_model_mapping[client_id] = model_type  # Update the model_type mapping

                if model_type == "taskA":
                    results_a.append((fit_res.parameters, fit_res.num_examples, fit_res.metrics))
                elif model_type == "taskB":
                    results_b.append((fit_res.parameters, fit_res.num_examples, fit_res.metrics))
                else:
                    # Handle unknown model_type if necessary
                    continue
            
            # Aggregate parameters for taskA
            if results_a:
                self.parameters_a = self.aggregate_parameters(results_a, "taskA")
            
            # Aggregate parameters for taskB
            if results_b:
                self.parameters_b = self.aggregate_parameters(results_b, "taskB")
            
            # Combine aggregated metrics
            metrics_aggregated = {
                "taskA": {
                    "train_loss": global_metrics["taskA"]["train_loss"][-1],
                    "train_accuracy": global_metrics["taskA"]["train_accuracy"][-1],
                    "val_loss": global_metrics["taskA"]["val_loss"][-1],
                    "val_accuracy": global_metrics["taskA"]["val_accuracy"][-1],
                },
                "taskB": {
                    "train_loss": global_metrics["taskB"]["train_loss"][-1],
                    "train_accuracy": global_metrics["taskB"]["train_accuracy"][-1],
                    "val_loss": global_metrics["taskB"]["val_loss"][-1],
                    "val_accuracy": global_metrics["taskB"]["val_accuracy"][-1],
                },
            }
            
            global currentRnd
            currentRnd += 1

            if currentRnd == num_rounds:
                generate_performance_graphs()
                generate_cpu_usage_graph()
                generate_total_time_graph()
                generate_training_time_graph()
                generate_communication_time_graph()
                client_registry.print_clients_info()
                print_final_results()
                
            # Return one of the parameter sets (as Flower expects a single Parameters object)
            # Alternatively, you might consider modifying Flower to handle multiple models
            return self.parameters_a, metrics_aggregated

    def aggregate_parameters(self, results, task_type):
        # Aggregate weights using weighted average based on number of examples
        total_examples = sum([num_examples for _, num_examples, _ in results])
        new_weights = None

        metrics = []
        for client_params, num_examples, client_metrics in results:
            client_weights = parameters_to_ndarrays(client_params)
            weight = num_examples / total_examples
            if new_weights is None:
                new_weights = [w * weight for w in client_weights]
            else:
                new_weights = [nw + w * weight for nw, w in zip(new_weights, client_weights)]
            metrics.append((num_examples, client_metrics))

        # Aggregate metrics
        weighted_average_global(metrics, task_type)

        return ndarrays_to_parameters(new_weights)

    def configure_evaluate(
        self,
        server_round: int,
        parameters: Parameters,
        client_manager: ClientManager,
    ) -> List[Tuple[ClientProxy, EvaluateIns]]:
        # Implement if necessary
        return []

    def aggregate_evaluate(
        self,
        server_round: int,
        results: List[Tuple[ClientProxy, EvaluateRes]],
        failures: List[BaseException],
    ) -> Optional[float]:
        # Implement if necessary
        return None

    def evaluate(
        self,
        server_round: int,
        parameters: Parameters,
    ) -> Optional[Tuple[float, Dict[str, Scalar]]]:
        # Implement if necessary
        return None

def server_fn(context: Context):
    strategy = MultiModelStrategy(initial_parameters_a=parametersA, initial_parameters_b=parametersB)
    server_config = ServerConfig(num_rounds=num_rounds)
    return ServerAppComponents(strategy=strategy, config=server_config)

app = ServerApp(server_fn=server_fn)

if __name__ == "__main__":
    app.run(server_address="0.0.0.0:8080", config=ServerConfig(num_rounds=num_rounds))


Apclient
import platform
import psutil
import random
from datetime import datetime

class ClientRegistry:
    def __init__(self):
        self.registry = {}

    def get_client_info(self):

        cpu_count = psutil.cpu_count(logical=False)  # Physical cores

        """Gathers system information about the client."""
        info = {
            "platform": platform.system(),
            "platform_version": platform.version(),
            "architecture": platform.machine(),
            "cpu": platform.processor(),
            "cpu_count": psutil.cpu_count(logical=False),  # Physical cores
            "cpu_threads": psutil.cpu_count(logical=True),  # Logical threads
            "ram_total": psutil.virtual_memory().total / (1024 ** 3),  # Total RAM in GB
            "ram_available": psutil.virtual_memory().available / (1024 ** 3),  # Available RAM in GB
            "python_version": platform.python_version(),
        }
        return info

    def register_client(self, cid, resource_info):
        """Registers a new client with its cid and detailed system information."""
        client_info = self.get_client_info()
        self.registry[cid] = {
            'resources': resource_info,
            'system_info': client_info,  # Save the system info
            'active': True,
            'last_update': datetime.now()
        }
        print(f"Client {cid} registered with resources: {resource_info}")
        print(f"System info: {client_info}")

    def get_client_model(self, cid):
        """Returns the model assigned to the client (taskA or taskB) based on cid."""
        if cid in self.registry:
            return self.registry[cid]['resources'].get("model_type", "taskA")
        else:
            # Assegnazione casuale del model_type
            model_type = random.choice(["taskA", "taskB"])
            # Registrare il client con il model_type assegnato
            self.register_client(cid, {"model_type": model_type})
            return model_type

    def update_client(self, cid, status, last_update=None):
        """Updates the status of the client."""
        if cid in self.registry:
            self.registry[cid]['active'] = status
            self.registry[cid]['last_update'] = last_update or datetime.now()

    def get_active_clients(self):
        """Returns the list of active clients."""
        active_clients = {cid: info for cid, info in self.registry.items() if info['active']}
        print(f"Active clients: {list(active_clients.keys())}")
        return active_clients

    def is_registered(self, cid):
        """Check if a client is already registered."""
        return cid in self.registry

    def print_clients_info(self):
        """Prints the list of clients with their respective system information."""
        active_clients = self.get_active_clients()
        if active_clients:
            for i, (cid, client_info) in enumerate(active_clients.items(), 1):
                system_info = client_info['system_info']
                print(f"Client {i}: {cid}")
                for key, value in system_info.items():
                    print(f"  {key}: {value}")
                print()  # Newline for readability
        else:
            print("No active clients registered.")